{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain.document_loaders.unstructured import UnstructuredFileLoader\n",
    "from paddleocr import PaddleOCR\n",
    "import os\n",
    "import fitz\n",
    "import nltk\n",
    "# from configs.model_config import NLTK_DATA_PATH\n",
    "\n",
    "NLTK_DATA_PATH = './nltk_data'\n",
    "\n",
    "nltk.data.path = [NLTK_DATA_PATH] + nltk.data.path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnstructuredPaddlePDFLoader(UnstructuredFileLoader):\n",
    "    def __get_elements(self)->List:\n",
    "        def pdf_or_txt(filepath,dir_path = 'tmp_files'):\n",
    "            full_dir_path = os.path.join(os.path.dirname(filepath), dir_path)\n",
    "            if not os.path.exists(full_dir_path):\n",
    "                os.makedirs(full_dir_path)\n",
    "            ocr = PaddleOCR(use_angle_cls=True, lang=\"ch\", use_gpu=False, show_log=False)\n",
    "            doc = fitz.open(filepath)\n",
    "            txt_file_path = os.path.join(full_dir_path, f\"{os.path.split(filepath)[-1]}.txt\")\n",
    "            img_name = os.path.join(full_dir_path, 'tmp.png')\n",
    "            with open(txt_file_path,'w',encoding='utf-8') as fout:\n",
    "                for i in range(doc.page_count):\n",
    "                    page = doc[i]\n",
    "                    text = page.get_text(\"\")\n",
    "                    fout.write(text)\n",
    "                    fout.write('\\n')\n",
    "\n",
    "                    img_list = page.get_image()\n",
    "                    for img in img_list:\n",
    "                        pix = fitz.Pixmap(doc, img[0])\n",
    "                        if pix.n - pix.alpha >=4:\n",
    "                            pix = fitz.Pixmap(fitz.csRGB, pix)\n",
    "                        pix.save(img_name)\n",
    "                        \n",
    "                        result = ocr.ocr(img_name)\n",
    "                        ocr_result = [i[1][0] for line in result for i in line]\n",
    "                        fout.write('\\n'.join(ocr_result))\n",
    "            if os.path.exists(img_name):\n",
    "                os.remove(img_name)\n",
    "            return txt_file_path\n",
    "\n",
    "        txt_file_path = pdf_or_txt(self.file_path)\n",
    "        from unstructured.partition.text import partition_text\n",
    "        return partition_text(file_name = txt_file_path,**self.unstructured_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-07-03 15:44:26,737] [ WARNING] pdf.py:149 - detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with the fast strategy.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='VisualGLM\\n\\n\\n\\n6B的原理与微调\\n\\n报告⼈：丁    铭\\n\\n清华⼤学计算机系\\n\\n智谱AI\\n\\nhttps://github.com/THUDM/\\n\\nVisualGLM\\n\\n\\n\\n6B\\n\\n大纲\\n\\n多模态预训练背景\\n\\n– 多模态预训练思路一（CogView思路）\\n\\n图像作为一种特殊的语言\\n\\n– 多模态预训练思路二（VisualGLM思路）\\n\\n图像对齐到预训练语言模型\\n\\n– ChatGLM\\n\\n\\n\\n6B的简单介绍\\n\\nVisualGLM\\n\\n– 训练过程\\n\\n– 安装与使用\\n\\n– 使用sat库进行微调\\n\\n未来展望\\n\\n2\\n\\n背景：多模态预训练\\n\\n以BERT/GPT为代表的自回归模型展现出强大的理解生成能力\\n\\n如何将这种成功“复制”到视觉/跨模态领域？\\n\\n以DALL-E、CogView、BEiT等为代表的模型的思路为：\\n\\n– 将图像当成一种语言进行预训练\\n\\n– 离散化\\x00token\\n\\n优势：\\n\\n– 图像为一等公民，同时进行图像生成和理解\\n\\n劣势：\\n\\ncogview\\n\\n– 离散化有损，降低token利用率，需要\\x00>1000tokens\\x00来描述一张256分辨率的图\\n\\n3\\n\\n背景：多模态预训练\\n\\n实际上，人类只对少量的视觉语义信息感兴趣\\n\\n如何提升效率，并充分利用语言模型？\\n\\nBLIP2提供了一种思路：\\n\\n– 将图像特征对齐到预训练语言模型\\n\\n优势：\\n\\n– 充分利用语言模型\\n\\n– 无缝接合原有的多轮对话能力\\n\\n劣势：\\n\\n– 提取图像语义特征损失底层信息\\n\\n4\\n\\n背景：ChatGLM\\n\\n\\n\\n6B\\n\\nChatGLM-6B 是一个开源的、具有62亿参数的中英双语语言模型。\\n\\n自3月14日发布之后一个月内，在 Github\\x00上获得了\\x002.1\\x00万\\x00star，全球\\n\\n下载量超过\\x00100\\x00万，登上Github Trending\\x00榜第一、Hugging Face\\n\\nTrending\\x00榜第一。\\n\\n海量中英文本\\n\\n文本预训练\\n\\nGLM\\n\\n\\n\\n130B\\n\\n指示\\n\\n\\n\\n回复配对\\n\\n数据\\n\\n指令微调\\n\\n反馈自助\\n\\n人类偏好数据\\n\\n人类反馈强化学习\\n\\nChatGLM\\n\\n5\\n\\nVisualGLM\\n\\n\\n\\n6B\\n\\n如何让ChatGLM拥有图像识别的能\\n\\n力？\\n\\n\\n\\n通用领域\\n\\n\\n\\n中英双语\\n\\n\\n\\n与正常语言问答顺畅融合\\n\\n基本思路：\\n\\n\\n\\n通过中间模块构建起预训练视觉和语言\\n\\n模型的桥梁\\n\\n中英双语图文数据大规模预训练\\n\\n高质量指令数据微调\\n\\n与纯语言类似的技术方案\\n\\nBLIP2、MiniGPT4等工作类似思路\\n\\n6\\n\\nVisualGLM\\n\\n\\n\\n6B\\x00训练\\n\\n模型架构： • ViT + QFormer + ChatGLM-6B • 几乎冻结 ViT\\x00和\\x00ChatGLM\\x00参数\\n\\n防止灾难性遗忘\\n\\n预训练：QFormer\\x00和\\x00Vit Lora进行学习\\n\\n微调：QFormer\\x00和\\x00ChatGLM Lora进行学\\n\\n习\\n\\n训练目标：\\n\\n\\n\\n自回归损失（根据图像生成正确的文本）\\n\\n\\n\\n对比损失（输入ChatGLM的视觉特征与对\\n\\n应文本的语义特征对齐）\\n\\n训练数据：\\n\\n\\n\\nCogView工作积累的30M中文图文对\\n\\n\\n\\n精选LAION+CC12M的100M英文图文对\\n\\n\\n\\n来自其他工作和数据集的视觉问答指令微调\\n\\n数据集\\n\\n自己构建的高质量视觉问答指令微调数据集\\n\\n7\\n\\n多轮对话\\n\\n\\n\\n效果\\n\\nVisualGLM\\n\\n\\n\\n6B能够\\n\\n场景描述\\n\\n进一步追问\\n\\n联系知识\\n\\n8\\n\\n多轮对话\\n\\n\\n\\n效果\\n\\nVisualGLM\\n\\n\\n\\n6B能够\\n\\n联系常识\\n\\n发现异常\\n\\n做出解释\\n\\n9\\n\\n效果举例\\n\\n10\\n\\n模型调用：sat实现\\n\\nVisualGLM\\n\\n\\n\\n6B使用SwissArmyTransformer（sat）库开发，这个库对于Transformer变体的代码撰写和训\\n\\n练非常方便， VisualGLM\\n\\n\\n\\n6B中包含Vit、Qformer、ChatGLM\\n\\n\\n\\n6B三种不同的Transformer，使用sat可以\\n\\n极大降低开发难度。\\n\\nimport argparse from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True) from model import chat, VisualGLMModel model, model_args = VisualGLMModel.from_pretrained(\\'visualglm-6b\\', args=argparse.Namespace(fp16=True, skip_init=True)) from sat.model.mixins import CachedAutoregressiveMixin model.add_mixin(\\'auto-regressive\\', CachedAutoregressiveMixin()) image_path = \"your image path or URL” response, history, cache_image = chat(image_path, model, tokenizer, \"描述这张图片。\", history=[]) print(response) response, history, cache_image = chat(None, model, tokenizer, \"这张图片可能是在什么场所 拍摄的？\", history=history, image=cache_image) print(response)\\n\\nHuggingface实现实际上是sat套壳\\n\\n11\\n\\n模型推理需要的资源\\n\\n支持环境：\\n\\n\\n\\nLinux，无特殊需求\\n\\n\\n\\nWindows，需跳过或正确按照deepspeed\\n\\n\\n\\nMac，需要有mps显卡或者使用cpu推理（需要代码中device稍作修改）\\n\\nfp16\\n\\nquant 8bit\\n\\nquant 4bit\\n\\nGPU显存占用（batch=1）\\n\\n16 GB\\n\\n11.2 GB\\n\\n8.7 GB\\n\\n12\\n\\n微调\\n\\n多模态任务分布广、种类多，预训练往往不能面面俱到。\\x00这里我们提供了一个\\n\\n小样本微调的例子，使用20张标注图增强模型回答“背景”问题的能力。\\x00\\n\\nbash finetune/finetune_visualglm.sh 目前支持三种方式的微调：\\x00\\n\\n– LoRA：样例中为ChatGLM模型的第0层和第14层加入了rank=10的LoRA微调，可以根据具体情\\n\\n景和数据量调整\\n\\n\\n\\n\\n\\nlayer_range和\\n\\n\\n\\n\\n\\nlora_rank参数。\\n\\n– QLoRA：如果资源有限，可以考虑使用bash finetune/finetune_visualglm_qlora.sh，QLoRA将\\n\\nChatGLM的线性层进行了4\\n\\n\\n\\nbit量化，只需要9.8GB显存即可微调。\\n\\n– P\\n\\n\\n\\ntuning：可以将\\n\\n\\n\\n\\n\\nuse_lora替换为\\n\\n\\n\\n\\n\\nuse_ptuning，不过不推荐使用，除非模型应用场景非常固\\n\\n定。\\n\\n训练好以后可以使用如下命令推理：\\x00\\n\\npython cli_demo.py --from_pretrained your_checkpoint_path --prompt_zh 这张图片的\\n\\n背景里有什么内容？\\x00微调前后的效果对比微调需要安装deepspeed库，目前本流\\n\\n程仅支持linux系统，更多的样例说明和Windows系统的流程说明将在近期完成。\\n\\n13\\n\\n不同微调方案需要的资源\\n\\n默认训练脚本中batch\\x00size较大，如果显存较小需要传入更小的batch\\x00size值\\x00\\n\\n如果想在资源有限的情况下增大batch\\x00size，可以在bash里加入—gradient-accumulation-steps参数\\n\\nLoRA\\n\\nQLoRA\\n\\nP\\n\\n\\n\\ntuning\\n\\nGPU显存占用（batch=1）\\n\\n17.8 GB\\n\\n9.8 GB\\n\\n18.1 GB\\n\\n14\\n\\nLora merge\\n\\n15\\n\\n未来展望\\n\\n中文OCR能力\\n\\n表格理解能力\\n\\n高分辨率图像\\n\\n更准确的描述\\n\\n更好的遵循指令\\n\\nWe’re hiring. Join Us via siya.xiong@zhipuai.cn\\n\\n16' metadata={'source': './database/VisualGLM微调.pdf'}\n"
     ]
    }
   ],
   "source": [
    "filepath =  \"./database/VisualGLM微调.pdf\"\n",
    "loader = UnstructuredPaddlePDFLoader(filepath)\n",
    "docs = loader.load()\n",
    "for doc in docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "docs[0].page_content =re.sub(r\"\\n{1,}\", r\"\", docs[0].page_content)\n",
    "docs[0].page_content =re.sub(r\"\\s\", r\"\", docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VisualGLM6B的原理与微调报告⼈：丁铭清华⼤学计算机系智谱AIhttps://github.com/THUDM/VisualGLM6B大纲多模态预训练背景–多模态预训练思路一（CogView思路）图像作为一种特殊的语言–多模态预训练思路二（VisualGLM思路）图像对齐到预训练语言模型–ChatGLM6B的简单介绍VisualGLM–训练过程–安装与使用–使用sat库进行微调未来展望2背景：多模态预训练以BERT/GPT为代表的自回归模型展现出强大的理解生成能力如何将这种成功“复制”到视觉/跨模态领域？以DALL-E、CogView、BEiT等为代表的模型的思路为：–将图像当成一种语言进行预训练–离散化\\x00token优势：–图像为一等公民，同时进行图像生成和理解劣势：cogview–离散化有损，降低token利用率，需要\\x00>1000tokens\\x00来描述一张256分辨率的图3背景：多模态预训练实际上，人类只对少量的视觉语义信息感兴趣如何提升效率，并充分利用语言模型？BLIP2提供了一种思路：–将图像特征对齐到预训练语言模型优势：–充分利用语言模型–无缝接合原有的多轮对话能力劣势：–提取图像语义特征损失底层信息4背景：ChatGLM6BChatGLM-6B是一个开源的、具有62亿参数的中英双语语言模型。自3月14日发布之后一个月内，在Github\\x00上获得了\\x002.1\\x00万\\x00star，全球下载量超过\\x00100\\x00万，登上GithubTrending\\x00榜第一、HuggingFaceTrending\\x00榜第一。海量中英文本文本预训练GLM130B指示回复配对数据指令微调反馈自助人类偏好数据人类反馈强化学习ChatGLM5VisualGLM6B如何让ChatGLM拥有图像识别的能力？通用领域中英双语与正常语言问答顺畅融合基本思路：通过中间模块构建起预训练视觉和语言模型的桥梁中英双语图文数据大规模预训练高质量指令数据微调与纯语言类似的技术方案BLIP2、MiniGPT4等工作类似思路6VisualGLM6B\\x00训练模型架构：•ViT+QFormer+ChatGLM-6B•几乎冻结ViT\\x00和\\x00ChatGLM\\x00参数防止灾难性遗忘预训练：QFormer\\x00和\\x00VitLora进行学习微调：QFormer\\x00和\\x00ChatGLMLora进行学习训练目标：自回归损失（根据图像生成正确的文本）对比损失（输入ChatGLM的视觉特征与对应文本的语义特征对齐）训练数据：CogView工作积累的30M中文图文对精选LAION+CC12M的100M英文图文对来自其他工作和数据集的视觉问答指令微调数据集自己构建的高质量视觉问答指令微调数据集7多轮对话效果VisualGLM6B能够场景描述进一步追问联系知识8多轮对话效果VisualGLM6B能够联系常识发现异常做出解释9效果举例10模型调用：sat实现VisualGLM6B使用SwissArmyTransformer（sat）库开发，这个库对于Transformer变体的代码撰写和训练非常方便，VisualGLM6B中包含Vit、Qformer、ChatGLM6B三种不同的Transformer，使用sat可以极大降低开发难度。importargparsefromtransformersimportAutoTokenizertokenizer=AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\",trust_remote_code=True)frommodelimportchat,VisualGLMModelmodel,model_args=VisualGLMModel.from_pretrained(\\'visualglm-6b\\',args=argparse.Namespace(fp16=True,skip_init=True))fromsat.model.mixinsimportCachedAutoregressiveMixinmodel.add_mixin(\\'auto-regressive\\',CachedAutoregressiveMixin())image_path=\"yourimagepathorURL”response,history,cache_image=chat(image_path,model,tokenizer,\"描述这张图片。\",history=[])print(response)response,history,cache_image=chat(None,model,tokenizer,\"这张图片可能是在什么场所拍摄的？\",history=history,image=cache_image)print(response)Huggingface实现实际上是sat套壳11模型推理需要的资源支持环境：Linux，无特殊需求Windows，需跳过或正确按照deepspeedMac，需要有mps显卡或者使用cpu推理（需要代码中device稍作修改）fp16quant8bitquant4bitGPU显存占用（batch=1）16GB11.2GB8.7GB12微调多模态任务分布广、种类多，预训练往往不能面面俱到。\\x00这里我们提供了一个小样本微调的例子，使用20张标注图增强模型回答“背景”问题的能力。\\x00bashfinetune/finetune_visualglm.sh目前支持三种方式的微调：\\x00–LoRA：样例中为ChatGLM模型的第0层和第14层加入了rank=10的LoRA微调，可以根据具体情景和数据量调整layer_range和lora_rank参数。–QLoRA：如果资源有限，可以考虑使用bashfinetune/finetune_visualglm_qlora.sh，QLoRA将ChatGLM的线性层进行了4bit量化，只需要9.8GB显存即可微调。–Ptuning：可以将use_lora替换为use_ptuning，不过不推荐使用，除非模型应用场景非常固定。训练好以后可以使用如下命令推理：\\x00pythoncli_demo.py--from_pretrainedyour_checkpoint_path--prompt_zh这张图片的背景里有什么内容？\\x00微调前后的效果对比微调需要安装deepspeed库，目前本流程仅支持linux系统，更多的样例说明和Windows系统的流程说明将在近期完成。13不同微调方案需要的资源默认训练脚本中batch\\x00size较大，如果显存较小需要传入更小的batch\\x00size值\\x00如果想在资源有限的情况下增大batch\\x00size，可以在bash里加入—gradient-accumulation-steps参数LoRAQLoRAPtuningGPU显存占用（batch=1）17.8GB9.8GB18.1GB14Loramerge15未来展望中文OCR能力表格理解能力高分辨率图像更准确的描述更好的遵循指令We’rehiring.JoinUsviasiya.xiong@zhipuai.cn16'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='VisualGLM6B的原理与微调报告⼈：丁铭清华⼤学计算机系智谱AIhttps://github.com/THUDM/VisualGLM6B大纲多模态预训练背景–多模态预训练思路一（CogView思路）图像作为一种特殊的语言–多模态预训练思路二（VisualGLM思路）图像对齐到预训练语言模型–ChatGLM6B的简单介绍VisualGLM–训练过程–安装与使用–使用sat库进行微调未来展望2背景：多模态预训练以BERT/GPT为代表的自回归模型展现出强大的理解生成能力如何将这种成功“复制”到视觉/跨模态领域？以DALL-E、CogView、BEiT等为代表的模型的思路为：–将图像当成一种语言进行预训练–离散化\\x00token优势：–图像为一等公民，同时进行图像生成和理解劣势：cogview–离散化有损，降低token利用率，需要\\x00>1000tokens\\x00来描述一张256分辨率的图3背景：多模态预训练实际上，人类只对少量的视觉语义信息感兴趣如何提升效率，并充分利用语言模型？BLIP2提供了一种思路：–将图像特征对齐到预训练语言模型优势：–充分利用语言模型–无缝接合原有的多轮对话能力劣势：–提取图像语义特征损失底层信息4背景：ChatGLM6BChatGLM-6B是一个开源的、具有62亿参数的中英双语语言模型。自3月14日发布之后一个月内，在Github\\x00上获得了\\x002.1\\x00万\\x00star，全球下载量超过\\x00100\\x00万，登上GithubTrending\\x00榜第一、HuggingFaceTrending\\x00榜第一。海量中英文本文本预训练GLM130B指示回复配对数据指令微调反馈自助人类偏好数据人类反馈强化学习ChatGLM5VisualGLM6B如何让ChatGLM拥有图像识别的能力？通用领域中英双语与正常语言问答顺畅融合基本思路：通过中间模块构建起预训练视觉和语言模型的桥梁中英双语图文数据大规模预训练高质量指令数据微调与纯语言类似的技术方案BLIP2、MiniGPT4等工作类似思路6VisualGLM6B\\x00训练模型架构：•ViT+QFormer+ChatGLM-6B•几乎冻结ViT\\x00和\\x00ChatGLM\\x00参数防止灾难性遗忘预训练：QFormer\\x00和\\x00VitLora进行学习微调：QFormer\\x00和\\x00ChatGLMLora进行学习训练目标：自回归损失（根据图像生成正确的文本）对比损失（输入ChatGLM的视觉特征与对应文本的语义特征对齐）训练数据：CogView工作积累的30M中文图文对精选LAION+CC12M的100M英文图文对来自其他工作和数据集的视觉问答指令微调数据集自己构建的高质量视觉问答指令微调数据集7多轮对话效果VisualGLM6B能够场景描述进一步追问联系知识8多轮对话效果VisualGLM6B能够联系常识发现异常做出解释9效果举例10模型调用：sat实现VisualGLM6B使用SwissArmyTransformer（sat）库开发，这个库对于Transformer变体的代码撰写和训练非常方便，VisualGLM6B中包含Vit、Qformer、ChatGLM6B三种不同的Transformer，使用sat可以极大降低开发难度。importargparsefromtransformersimportAutoTokenizertokenizer=AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\",trust_remote_code=True)frommodelimportchat,VisualGLMModelmodel,model_args=VisualGLMModel.from_pretrained(\\'visualglm-6b\\',args=argparse.Namespace(fp16=True,skip_init=True))fromsat.model.mixinsimportCachedAutoregressiveMixinmodel.add_mixin(\\'auto-regressive\\',CachedAutoregressiveMixin())image_path=\"yourimagepathorURL”response,history,cache_image=chat(image_path,model,tokenizer,\"描述这张图片。\",history=[])print(response)response,history,cache_image=chat(None,model,tokenizer,\"这张图片可能是在什么场所拍摄的？\",history=history,image=cache_image)print(response)Huggingface实现实际上是sat套壳11模型推理需要的资源支持环境：Linux，无特殊需求Windows，需跳过或正确按照deepspeedMac，需要有mps显卡或者使用cpu推理（需要代码中device稍作修改）fp16quant8bitquant4bitGPU显存占用（batch=1）16GB11.2GB8.7GB12微调多模态任务分布广、种类多，预训练往往不能面面俱到。\\x00这里我们提供了一个小样本微调的例子，使用20张标注图增强模型回答“背景”问题的能力。\\x00bashfinetune/finetune_visualglm.sh目前支持三种方式的微调：\\x00–LoRA：样例中为ChatGLM模型的第0层和第14层加入了rank=10的LoRA微调，可以根据具体情景和数据量调整layer_range和lora_rank参数。–QLoRA：如果资源有限，可以考虑使用bashfinetune/finetune_visualglm_qlora.sh，QLoRA将ChatGLM的线性层进行了4bit量化，只需要9.8GB显存即可微调。–Ptuning：可以将use_lora替换为use_ptuning，不过不推荐使用，除非模型应用场景非常固定。训练好以后可以使用如下命令推理：\\x00pythoncli_demo.py--from_pretrainedyour_checkpoint_path--prompt_zh这张图片的背景里有什么内容？\\x00微调前后的效果对比微调需要安装deepspeed库，目前本流程仅支持linux系统，更多的样例说明和Windows系统的流程说明将在近期完成。13不同微调方案需要的资源默认训练脚本中batch\\x00size较大，如果显存较小需要传入更小的batch\\x00size值\\x00如果想在资源有限的情况下增大batch\\x00size，可以在bash里加入—gradient-accumulation-steps参数LoRAQLoRAPtuningGPU显存占用（batch=1）17.8GB9.8GB18.1GB14Loramerge15未来展望中文OCR能力表格理解能力高分辨率图像更准确的描述更好的遵循指令We’rehiring.JoinUsviasiya.xiong@zhipuai.cn16', metadata={'source': './database/VisualGLM微调.pdf'})]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textsplitter import ChineseTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-07-03 15:44:38,459] [ WARNING] pdf.py:149 - detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with the fast strategy.\n"
     ]
    }
   ],
   "source": [
    "textsplitter = ChineseTextSplitter(pdf=True, sentence_size=sentence_size)\n",
    "docs = loader.load_and_split(textsplitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='VisualGLM 6B的原理与微调  报告⼈：丁    铭  清华⼤学计算机系  智谱AI  https://github.', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='com/THUDM/  VisualGLM 6B  大纲  多模态预训练背景  – 多模态预训练思路一（CogView思路）  图像作为一种特殊的语言  – 多模态预训练思路二（VisualGLM思路）  图像对齐到预训练语言模型  – ChatGLM 6B的简单介绍  VisualGLM  – 训练过程  – 安装与使用  – 使用sat库进行微调  未来展望  2  背景：多模态预训练  以BERT/GPT为代表的自回归模型展现出强大的理解生成能力  如何将这种成功“复制”到视觉/跨模态领域？', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='  以DALL-E、CogView、BEiT等为代表的模型的思路为：  – 将图像当成一种语言进行预训练  – 离散化\\x00token  优势：  – 图像为一等公民，同时进行图像生成和理解  劣势：  cogview  – 离散化有损，降低token利用率，需要\\x00>1000tokens\\x00来描述一张256分辨率的图  3  背景：多模态预训练  实际上，人类只对少量的视觉语义信息感兴趣  如何提升效率，并充分利用语言模型？', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='  BLIP2提供了一种思路：  – 将图像特征对齐到预训练语言模型  优势：  – 充分利用语言模型  – 无缝接合原有的多轮对话能力  劣势：  – 提取图像语义特征损失底层信息  4  背景：ChatGLM 6B  ChatGLM-6B 是一个开源的、具有62亿参数的中英双语语言模型。', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='  自3月14日发布之后一个月内，在 Github\\x00上获得了\\x002.', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='1\\x00万\\x00star，全球  下载量超过\\x00100\\x00万，登上Github Trending\\x00榜第一、Hugging Face  Trending\\x00榜第一。', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='  海量中英文本  文本预训练  GLM 130B  指示 回复配对  数据  指令微调  反馈自助  人类偏好数据  人类反馈强化学习  ChatGLM  5  VisualGLM 6B  如何让ChatGLM拥有图像识别的能  力？', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content=' 通用领域 中英双语 与正常语言问答顺畅融合  ', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='基本思路： 通过中间模块构建起预训练视觉和语言  ', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='模型的桥梁  ', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='中英双语图文数据大规模预训练  ', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='高质量指令数据微调  ', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='与纯语言类似的技术方案  ', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='BLIP2、MiniGPT4等工作类似思路  ', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='6  ', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='VisualGLM 6B\\x00训练  ', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='模型架构： • ViT + QFormer + ChatGLM-6B • 几乎冻结 ViT\\x00和\\x00ChatGLM\\x00参数  ', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='防止灾难性遗忘  ', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='预训练：QFormer\\x00和\\x00Vit Lora进行学习  ', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='微调：QFormer\\x00和\\x00ChatGLM Lora进行学  ', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='习  ', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='训练目标： 自回归损失（根据图像生成正确的文本） 对比损失（输入ChatGLM的视觉特征与对  ', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='应文本的语义特征对齐）  ', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='训练数据： CogView工作积累的30M中文图文对 精选LAION+CC12M的100M英文图文对 来自其他工作和数据集的视觉问答指令微调  ', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='数据集  ', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='自己构建的高质量视觉问答指令微调数据集  ', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='7  ', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='多轮对话 效果  ', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='VisualGLM 6B能够  ', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='场景描述  ', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='进一步追问  ', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='联系知识  ', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='8  ', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='多轮对话 效果  ', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='VisualGLM 6B能够  ', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='联系常识  ', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='发现异常  ', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='做出解释  ', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='9  ', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='效果举例  ', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='10  ', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='模型调用：sat实现  ', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='VisualGLM 6B使用SwissArmyTransformer（sat）库开发，', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='这个库对于Transformer变体的代码撰写和训  练非常方便，', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content=' VisualGLM 6B中包含Vit、Qformer、ChatGLM 6B三种不同的Transformer，', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='使用sat可以  极大降低开发难度。', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='  import argparse from transformers import AutoTokenizer tokenizer = AutoTokenizer.', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True) from model import chat, VisualGLMModel model, model_args = VisualGLMModel.', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content=\"from_pretrained('visualglm-6b', args=argparse.\", metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='Namespace(fp16=True, skip_init=True)) from sat.', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='model.', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='mixins import CachedAutoregressiveMixin model.', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='add_mixin(\\'auto-regressive\\', CachedAutoregressiveMixin()) image_path = \"your image path or URL” response, history, cache_image = chat(image_path, model, tokenizer, \"描述这张图片。', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='\", history=[]) print(response) response, history, cache_image = chat(None, model, tokenizer, \"这张图片可能是在什么场所 拍摄的？', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='\", history=history, image=cache_image) print(response)  Huggingface实现实际上是sat套壳  11  模型推理需要的资源  支持环境： Linux，无特殊需求 Windows，需跳过或正确按照deepspeed Mac，需要有mps显卡或者使用cpu推理（需要代码中device稍作修改）  fp16  quant 8bit  quant 4bit  GPU显存占用（batch=1）  16 GB  11.', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='2 GB  8.', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='7 GB  12  微调  多模态任务分布广、种类多，预训练往往不能面面俱到。', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='\\x00这里我们提供了一个  小样本微调的例子，使用20张标注图增强模型回答“背景”问题的能力。', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='\\x00  bash finetune/finetune_visualglm.', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='sh 目前支持三种方式的微调：\\x00  – LoRA：样例中为ChatGLM模型的第0层和第14层加入了rank=10的LoRA微调，可以根据具体情  景和数据量调整 layer_range和 lora_rank参数。', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='  – QLoRA：如果资源有限，可以考虑使用bash finetune/finetune_visualglm_qlora.', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='sh，QLoRA将  ChatGLM的线性层进行了4 bit量化，只需要9.', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='8GB显存即可微调。', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='  – P tuning：可以将 use_lora替换为 use_ptuning，不过不推荐使用，除非模型应用场景非常固  定。', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='  训练好以后可以使用如下命令推理：\\x00  python cli_demo.', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='py --from_pretrained your_checkpoint_path --prompt_zh 这张图片的  背景里有什么内容？', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='\\x00微调前后的效果对比微调需要安装deepspeed库，目前本流  程仅支持linux系统，更多的样例说明和Windows系统的流程说明将在近期完成。', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='  13  不同微调方案需要的资源  默认训练脚本中batch\\x00size较大，如果显存较小需要传入更小的batch\\x00size值\\x00  如果想在资源有限的情况下增大batch\\x00size，可以在bash里加入—gradient-accumulation-steps参数  LoRA  QLoRA  P tuning  GPU显存占用（batch=1）  17.', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='8 GB  9.', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='8 GB  18.', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='1 GB  14  Lora merge  15  未来展望  中文OCR能力  表格理解能力  高分辨率图像  更准确的描述  更好的遵循指令  We’re hiring.', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content=' Join Us via siya.', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='xiong@zhipuai.', metadata={'source': './database/VisualGLM微调.pdf'}),\n",
       " Document(page_content='cn  16', metadata={'source': './database/VisualGLM微调.pdf'})]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = input(\"Input your local knowledge folder path 请输入本地知识文件夹路径：\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = './database/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "filepath = './database/'\n",
    "file_list = [i for j in ['txt','pdf','md','csv'] for i in glob.glob(filepath +\"*.\"+j) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./database/test.txt', './database/VisualGLM微调.pdf', './database/test.pdf']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
